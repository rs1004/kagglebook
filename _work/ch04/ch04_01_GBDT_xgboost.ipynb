{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../input/sample-data/train_preprocessed.csv')\n",
    "train_x = train.drop(['target'], axis=1)\n",
    "train_y = train['target']\n",
    "test_x = pd.read_csv('../../input/sample-data/test_preprocessed.csv')\n",
    "\n",
    "# 学習データを学習データとバリデーションデータに分ける\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量と目的変数をxgboostのデータ構造に変換する\n",
    "dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "dtest = xgb.DMatrix(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 0, 'eval_metric': 'logloss', 'booster': 'gbtree'}\n",
    "num_round = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.539452\teval-logloss:0.551324\n",
      "Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.449389\teval-logloss:0.471079\n",
      "[2]\ttrain-logloss:0.391917\teval-logloss:0.423298\n",
      "[3]\ttrain-logloss:0.349746\teval-logloss:0.389744\n",
      "[4]\ttrain-logloss:0.316009\teval-logloss:0.36525\n",
      "[5]\ttrain-logloss:0.291775\teval-logloss:0.345957\n",
      "[6]\ttrain-logloss:0.272171\teval-logloss:0.33411\n",
      "[7]\ttrain-logloss:0.257363\teval-logloss:0.322367\n",
      "[8]\ttrain-logloss:0.242918\teval-logloss:0.312858\n",
      "[9]\ttrain-logloss:0.227988\teval-logloss:0.306092\n",
      "[10]\ttrain-logloss:0.219927\teval-logloss:0.30211\n",
      "[11]\ttrain-logloss:0.208176\teval-logloss:0.294525\n",
      "[12]\ttrain-logloss:0.199738\teval-logloss:0.293724\n",
      "[13]\ttrain-logloss:0.19197\teval-logloss:0.28883\n",
      "[14]\ttrain-logloss:0.183975\teval-logloss:0.285978\n",
      "[15]\ttrain-logloss:0.176698\teval-logloss:0.280726\n",
      "[16]\ttrain-logloss:0.17195\teval-logloss:0.278298\n",
      "[17]\ttrain-logloss:0.167411\teval-logloss:0.27423\n",
      "[18]\ttrain-logloss:0.162944\teval-logloss:0.272659\n",
      "[19]\ttrain-logloss:0.153703\teval-logloss:0.267651\n",
      "[20]\ttrain-logloss:0.149668\teval-logloss:0.267236\n",
      "[21]\ttrain-logloss:0.145478\teval-logloss:0.265353\n",
      "[22]\ttrain-logloss:0.139245\teval-logloss:0.262933\n",
      "[23]\ttrain-logloss:0.135518\teval-logloss:0.25999\n",
      "[24]\ttrain-logloss:0.129673\teval-logloss:0.25442\n",
      "[25]\ttrain-logloss:0.125364\teval-logloss:0.253499\n",
      "[26]\ttrain-logloss:0.122164\teval-logloss:0.25228\n",
      "[27]\ttrain-logloss:0.119194\teval-logloss:0.25104\n",
      "[28]\ttrain-logloss:0.11656\teval-logloss:0.251121\n",
      "[29]\ttrain-logloss:0.11285\teval-logloss:0.24922\n",
      "[30]\ttrain-logloss:0.110201\teval-logloss:0.248924\n",
      "[31]\ttrain-logloss:0.108097\teval-logloss:0.247373\n",
      "[32]\ttrain-logloss:0.104877\teval-logloss:0.247012\n",
      "[33]\ttrain-logloss:0.103103\teval-logloss:0.247345\n",
      "[34]\ttrain-logloss:0.100786\teval-logloss:0.245673\n",
      "[35]\ttrain-logloss:0.098694\teval-logloss:0.246182\n",
      "[36]\ttrain-logloss:0.097324\teval-logloss:0.245982\n",
      "[37]\ttrain-logloss:0.094652\teval-logloss:0.246723\n",
      "[38]\ttrain-logloss:0.090638\teval-logloss:0.245961\n",
      "[39]\ttrain-logloss:0.089221\teval-logloss:0.244926\n",
      "[40]\ttrain-logloss:0.087088\teval-logloss:0.244369\n",
      "[41]\ttrain-logloss:0.085397\teval-logloss:0.243923\n",
      "[42]\ttrain-logloss:0.082631\teval-logloss:0.240358\n",
      "[43]\ttrain-logloss:0.080734\teval-logloss:0.239656\n",
      "[44]\ttrain-logloss:0.078772\teval-logloss:0.240678\n",
      "[45]\ttrain-logloss:0.076253\teval-logloss:0.239008\n",
      "[46]\ttrain-logloss:0.074709\teval-logloss:0.238337\n",
      "[47]\ttrain-logloss:0.073898\teval-logloss:0.237584\n",
      "[48]\ttrain-logloss:0.071563\teval-logloss:0.237871\n",
      "[49]\ttrain-logloss:0.069507\teval-logloss:0.237747\n",
      "[50]\ttrain-logloss:0.067408\teval-logloss:0.237199\n",
      "[51]\ttrain-logloss:0.066433\teval-logloss:0.237145\n",
      "[52]\ttrain-logloss:0.065583\teval-logloss:0.23701\n",
      "[53]\ttrain-logloss:0.064552\teval-logloss:0.237254\n",
      "[54]\ttrain-logloss:0.062928\teval-logloss:0.237239\n",
      "[55]\ttrain-logloss:0.061263\teval-logloss:0.234892\n",
      "[56]\ttrain-logloss:0.060131\teval-logloss:0.235731\n",
      "[57]\ttrain-logloss:0.059367\teval-logloss:0.235903\n",
      "[58]\ttrain-logloss:0.058458\teval-logloss:0.235928\n",
      "[59]\ttrain-logloss:0.057792\teval-logloss:0.236615\n",
      "[60]\ttrain-logloss:0.05657\teval-logloss:0.235475\n",
      "[61]\ttrain-logloss:0.05592\teval-logloss:0.235591\n",
      "[62]\ttrain-logloss:0.054876\teval-logloss:0.235959\n",
      "[63]\ttrain-logloss:0.054438\teval-logloss:0.235856\n",
      "[64]\ttrain-logloss:0.053755\teval-logloss:0.235899\n",
      "[65]\ttrain-logloss:0.05175\teval-logloss:0.232681\n",
      "[66]\ttrain-logloss:0.050434\teval-logloss:0.232004\n",
      "[67]\ttrain-logloss:0.049215\teval-logloss:0.230281\n",
      "[68]\ttrain-logloss:0.048795\teval-logloss:0.230287\n",
      "[69]\ttrain-logloss:0.047963\teval-logloss:0.231659\n",
      "[70]\ttrain-logloss:0.047003\teval-logloss:0.232147\n",
      "[71]\ttrain-logloss:0.045373\teval-logloss:0.231417\n",
      "[72]\ttrain-logloss:0.044504\teval-logloss:0.230584\n",
      "[73]\ttrain-logloss:0.044201\teval-logloss:0.2307\n",
      "[74]\ttrain-logloss:0.043043\teval-logloss:0.231243\n",
      "[75]\ttrain-logloss:0.042669\teval-logloss:0.230355\n",
      "[76]\ttrain-logloss:0.041712\teval-logloss:0.230431\n",
      "[77]\ttrain-logloss:0.041473\teval-logloss:0.230482\n",
      "[78]\ttrain-logloss:0.041027\teval-logloss:0.230812\n",
      "[79]\ttrain-logloss:0.03977\teval-logloss:0.230056\n",
      "[80]\ttrain-logloss:0.039085\teval-logloss:0.22998\n",
      "[81]\ttrain-logloss:0.038319\teval-logloss:0.228976\n",
      "[82]\ttrain-logloss:0.037982\teval-logloss:0.229831\n",
      "[83]\ttrain-logloss:0.037373\teval-logloss:0.229732\n",
      "[84]\ttrain-logloss:0.036931\teval-logloss:0.229318\n",
      "[85]\ttrain-logloss:0.035996\teval-logloss:0.229894\n",
      "[86]\ttrain-logloss:0.035435\teval-logloss:0.231077\n",
      "[87]\ttrain-logloss:0.034548\teval-logloss:0.230683\n",
      "[88]\ttrain-logloss:0.03401\teval-logloss:0.23058\n",
      "[89]\ttrain-logloss:0.0335\teval-logloss:0.231634\n",
      "[90]\ttrain-logloss:0.032835\teval-logloss:0.231748\n",
      "[91]\ttrain-logloss:0.032592\teval-logloss:0.232207\n",
      "[92]\ttrain-logloss:0.03179\teval-logloss:0.232642\n",
      "[93]\ttrain-logloss:0.031622\teval-logloss:0.232456\n",
      "[94]\ttrain-logloss:0.030994\teval-logloss:0.233482\n",
      "[95]\ttrain-logloss:0.030142\teval-logloss:0.233483\n",
      "[96]\ttrain-logloss:0.029563\teval-logloss:0.234358\n",
      "[97]\ttrain-logloss:0.029168\teval-logloss:0.234314\n",
      "[98]\ttrain-logloss:0.028899\teval-logloss:0.235039\n",
      "[99]\ttrain-logloss:0.028391\teval-logloss:0.234671\n",
      "[100]\ttrain-logloss:0.027808\teval-logloss:0.234265\n",
      "[101]\ttrain-logloss:0.027357\teval-logloss:0.234212\n",
      "Stopping. Best iteration:\n",
      "[81]\ttrain-logloss:0.038319\teval-logloss:0.228976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 学習の実行\n",
    "# watchlistには学習データおよびバリデーションデータをセットする\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "model = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.2290\n"
     ]
    }
   ],
   "source": [
    "# バリデーションデータでのスコアの確認\n",
    "va_pred = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n",
    "score = log_loss(va_y, va_pred)\n",
    "print(f'logloss: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### booster\n",
    "- ハイパーパラメータのboosterは変更できる\n",
    "  - gbtree: デフォルト\n",
    "  - gblinear: 線形モデルと同様の表現力になる。あまり使われない。\n",
    "  - dart: nn における dropout をしながら boosting する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.539452\teval-logloss:0.551324\n",
      "Multiple eval metrics have been passed: 'eval-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.449389\teval-logloss:0.471079\n",
      "[2]\ttrain-logloss:0.391917\teval-logloss:0.423298\n",
      "[3]\ttrain-logloss:0.349746\teval-logloss:0.389744\n",
      "[4]\ttrain-logloss:0.316009\teval-logloss:0.36525\n",
      "[5]\ttrain-logloss:0.291775\teval-logloss:0.345957\n",
      "[6]\ttrain-logloss:0.272171\teval-logloss:0.33411\n",
      "[7]\ttrain-logloss:0.257363\teval-logloss:0.322367\n",
      "[8]\ttrain-logloss:0.242918\teval-logloss:0.312858\n",
      "[9]\ttrain-logloss:0.227988\teval-logloss:0.306092\n",
      "[10]\ttrain-logloss:0.219927\teval-logloss:0.30211\n",
      "[11]\ttrain-logloss:0.208176\teval-logloss:0.294525\n",
      "[12]\ttrain-logloss:0.199738\teval-logloss:0.293724\n",
      "[13]\ttrain-logloss:0.19197\teval-logloss:0.28883\n",
      "[14]\ttrain-logloss:0.183975\teval-logloss:0.285978\n",
      "[15]\ttrain-logloss:0.176698\teval-logloss:0.280726\n",
      "[16]\ttrain-logloss:0.17195\teval-logloss:0.278298\n",
      "[17]\ttrain-logloss:0.167411\teval-logloss:0.27423\n",
      "[18]\ttrain-logloss:0.162944\teval-logloss:0.272659\n",
      "[19]\ttrain-logloss:0.153703\teval-logloss:0.267651\n",
      "[20]\ttrain-logloss:0.149668\teval-logloss:0.267236\n",
      "[21]\ttrain-logloss:0.145478\teval-logloss:0.265353\n",
      "[22]\ttrain-logloss:0.139245\teval-logloss:0.262933\n",
      "[23]\ttrain-logloss:0.135518\teval-logloss:0.25999\n",
      "[24]\ttrain-logloss:0.129673\teval-logloss:0.25442\n",
      "[25]\ttrain-logloss:0.125364\teval-logloss:0.253499\n",
      "[26]\ttrain-logloss:0.122164\teval-logloss:0.25228\n",
      "[27]\ttrain-logloss:0.119194\teval-logloss:0.25104\n",
      "[28]\ttrain-logloss:0.11656\teval-logloss:0.251121\n",
      "[29]\ttrain-logloss:0.11285\teval-logloss:0.24922\n",
      "[30]\ttrain-logloss:0.110201\teval-logloss:0.248924\n",
      "[31]\ttrain-logloss:0.108097\teval-logloss:0.247373\n",
      "[32]\ttrain-logloss:0.104877\teval-logloss:0.247012\n",
      "[33]\ttrain-logloss:0.103103\teval-logloss:0.247345\n",
      "[34]\ttrain-logloss:0.100786\teval-logloss:0.245673\n",
      "[35]\ttrain-logloss:0.098694\teval-logloss:0.246182\n",
      "[36]\ttrain-logloss:0.097324\teval-logloss:0.245982\n",
      "[37]\ttrain-logloss:0.094652\teval-logloss:0.246723\n",
      "[38]\ttrain-logloss:0.090638\teval-logloss:0.245961\n",
      "[39]\ttrain-logloss:0.089221\teval-logloss:0.244926\n",
      "[40]\ttrain-logloss:0.087088\teval-logloss:0.244369\n",
      "[41]\ttrain-logloss:0.085397\teval-logloss:0.243923\n",
      "[42]\ttrain-logloss:0.082631\teval-logloss:0.240358\n",
      "[43]\ttrain-logloss:0.080734\teval-logloss:0.239656\n",
      "[44]\ttrain-logloss:0.078772\teval-logloss:0.240678\n",
      "[45]\ttrain-logloss:0.076253\teval-logloss:0.239008\n",
      "[46]\ttrain-logloss:0.074709\teval-logloss:0.238337\n",
      "[47]\ttrain-logloss:0.073898\teval-logloss:0.237584\n",
      "[48]\ttrain-logloss:0.071563\teval-logloss:0.237871\n",
      "[49]\ttrain-logloss:0.069507\teval-logloss:0.237747\n",
      "[50]\ttrain-logloss:0.067408\teval-logloss:0.237199\n",
      "[51]\ttrain-logloss:0.066433\teval-logloss:0.237145\n",
      "[52]\ttrain-logloss:0.065583\teval-logloss:0.23701\n",
      "[53]\ttrain-logloss:0.064552\teval-logloss:0.237254\n",
      "[54]\ttrain-logloss:0.062928\teval-logloss:0.237239\n",
      "[55]\ttrain-logloss:0.061263\teval-logloss:0.234892\n",
      "[56]\ttrain-logloss:0.060131\teval-logloss:0.235731\n",
      "[57]\ttrain-logloss:0.059367\teval-logloss:0.235903\n",
      "[58]\ttrain-logloss:0.058458\teval-logloss:0.235928\n",
      "[59]\ttrain-logloss:0.057792\teval-logloss:0.236615\n",
      "[60]\ttrain-logloss:0.05657\teval-logloss:0.235475\n",
      "[61]\ttrain-logloss:0.05592\teval-logloss:0.235591\n",
      "[62]\ttrain-logloss:0.054876\teval-logloss:0.235959\n",
      "[63]\ttrain-logloss:0.054438\teval-logloss:0.235856\n",
      "[64]\ttrain-logloss:0.053755\teval-logloss:0.235899\n",
      "[65]\ttrain-logloss:0.05175\teval-logloss:0.232681\n",
      "[66]\ttrain-logloss:0.050434\teval-logloss:0.232004\n",
      "[67]\ttrain-logloss:0.049215\teval-logloss:0.230281\n",
      "[68]\ttrain-logloss:0.048795\teval-logloss:0.230287\n",
      "[69]\ttrain-logloss:0.047963\teval-logloss:0.231659\n",
      "[70]\ttrain-logloss:0.047003\teval-logloss:0.232147\n",
      "[71]\ttrain-logloss:0.045373\teval-logloss:0.231417\n",
      "[72]\ttrain-logloss:0.044504\teval-logloss:0.230584\n",
      "[73]\ttrain-logloss:0.044201\teval-logloss:0.2307\n",
      "[74]\ttrain-logloss:0.043043\teval-logloss:0.231243\n",
      "[75]\ttrain-logloss:0.042669\teval-logloss:0.230355\n",
      "[76]\ttrain-logloss:0.041712\teval-logloss:0.230431\n",
      "[77]\ttrain-logloss:0.041473\teval-logloss:0.230482\n",
      "[78]\ttrain-logloss:0.041027\teval-logloss:0.230812\n",
      "[79]\ttrain-logloss:0.03977\teval-logloss:0.230056\n",
      "[80]\ttrain-logloss:0.039085\teval-logloss:0.22998\n",
      "[81]\ttrain-logloss:0.038319\teval-logloss:0.228976\n",
      "[82]\ttrain-logloss:0.037982\teval-logloss:0.229831\n",
      "[83]\ttrain-logloss:0.037373\teval-logloss:0.229732\n",
      "[84]\ttrain-logloss:0.036931\teval-logloss:0.229318\n",
      "[85]\ttrain-logloss:0.035996\teval-logloss:0.229894\n",
      "[86]\ttrain-logloss:0.035435\teval-logloss:0.231077\n",
      "[87]\ttrain-logloss:0.034548\teval-logloss:0.230683\n",
      "[88]\ttrain-logloss:0.03401\teval-logloss:0.23058\n",
      "[89]\ttrain-logloss:0.0335\teval-logloss:0.231634\n",
      "[90]\ttrain-logloss:0.032835\teval-logloss:0.231748\n",
      "[91]\ttrain-logloss:0.032592\teval-logloss:0.232207\n",
      "[92]\ttrain-logloss:0.03179\teval-logloss:0.232642\n",
      "[93]\ttrain-logloss:0.031622\teval-logloss:0.232456\n",
      "[94]\ttrain-logloss:0.030994\teval-logloss:0.233482\n",
      "[95]\ttrain-logloss:0.030142\teval-logloss:0.233483\n",
      "[96]\ttrain-logloss:0.029563\teval-logloss:0.234358\n",
      "[97]\ttrain-logloss:0.029168\teval-logloss:0.234314\n",
      "[98]\ttrain-logloss:0.028899\teval-logloss:0.235039\n",
      "[99]\ttrain-logloss:0.028391\teval-logloss:0.234671\n",
      "[100]\ttrain-logloss:0.027808\teval-logloss:0.234265\n",
      "[101]\ttrain-logloss:0.027357\teval-logloss:0.234212\n",
      "Stopping. Best iteration:\n",
      "[81]\ttrain-logloss:0.038319\teval-logloss:0.228976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# booster = dartとしてみる\n",
    "\n",
    "# 特徴量と目的変数をxgboostのデータ構造に変換する\n",
    "dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "dtest = xgb.DMatrix(test_x)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "params = {'objective': 'binary:logistic', 'silent': 1, 'random_state': 0, 'eval_metric': 'logloss', 'booster': 'dart'}\n",
    "num_round = 500\n",
    "\n",
    "# 学習の実行\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "model = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.2290\n"
     ]
    }
   ],
   "source": [
    "# バリデーションデータでのスコアの確認\n",
    "va_pred = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n",
    "score = log_loss(va_y, va_pred)\n",
    "print(f'logloss: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⇒ log_lossは今回の例では同じ値になった。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
